{
 "metadata": {
  "name": "",
  "signature": "sha256:c57557e1121d4c7c3bfb9ab72927136b6c1f7be63da2e9849145a4db782b207a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "import csv\n",
      "import argparse\n",
      "import urllib\n",
      "import urllib2\n",
      "import json\n",
      "import httplib\n",
      "import datetime\n",
      "import random\n",
      "import os\n",
      "import pickle\n",
      "from datetime import timedelta\n",
      "import oauth2\n",
      "\n",
      "class TwitterData:\n",
      "    #start __init__\n",
      "    def __init__(self):\n",
      "        self.currDate = datetime.datetime.now()\n",
      "        self.weekDates = []\n",
      "        self.weekDates.append(self.currDate.strftime(\"%Y-%m-%d\"))\n",
      "        for i in range(1,7):\n",
      "            dateDiff = timedelta(days=-i)\n",
      "            newDate = self.currDate + dateDiff\n",
      "            self.weekDates.append(newDate.strftime(\"%Y-%m-%d\"))\n",
      "        #end loop\n",
      "    #end\n",
      "\n",
      "    #start getWeeksData\n",
      "    def getTwitterData(self, keyword, time):\n",
      "        self.weekTweets = []\n",
      "        if(time == 'lastweek'):\n",
      "            for i in range(0,6):\n",
      "                params = {'since': self.weekDates[i+1], 'until': self.weekDates[i]}\n",
      "                self.weekTweets[i] = self.getData(keyword, params)\n",
      "            #end loop\n",
      "            \n",
      "            #Write data to a pickle file\n",
      "            filename = 'data/weekTweets/weekTweets_'+urllib.unquote(keyword.replace(\"+\", \" \"))+'_'+str(int(random.random()*10000))+'.txt'\n",
      "            outfile = open(filename, 'wb')        \n",
      "            pickle.dump(self.weekTweets, outfile)        \n",
      "            outfile.close()\n",
      "        elif(time == 'today'):\n",
      "\t    maxTweets = 50\n",
      "       \t    url = 'https://api.twitter.com/1.1/search/tweets.json?'    \n",
      "            data = {'q': keyword, 'lang': 'en', 'result_type': 'recent', 'count': maxTweets, 'include_entities': 0}\n",
      "\t\n",
      "            #Add if additional params are passed\n",
      "            #if params:\n",
      "            #\tfor key, value in params.iteritems():\n",
      "             #   \tdata[key] = value\n",
      "        \n",
      "            url += urllib.urlencode(data)\n",
      "        \n",
      "            response = self.oauth_req(url)\n",
      "            jsonData = json.loads(response)\n",
      "            tweets = []\n",
      "\t    print \"here\"\n",
      "            if 'errors' in jsonData:\n",
      "            \tprint \"API Error\"\n",
      "            \tprint jsonData['errors']\n",
      "            else:\n",
      "            \tfor item in jsonData['statuses']:\n",
      "            \t    self.weekTweets.append(item['text'])   \n",
      "            \t#for i in range(0,1):\n",
      "                #\tparams = {'since': self.weekDates[i+1], 'until': self.weekDates[i]}\n",
      "                #\tself.weekTweets.append(self.getData(keyword, params))\n",
      "            #end loop\n",
      "            return self.weekTweets\n",
      "    '''\n",
      "        inpfile = open('data/weekTweets/weekTweets_obama_7303.txt')\n",
      "        self.weekTweets = pickle.load(inpfile)\n",
      "        inpfile.close()\n",
      "        return self.weekTweets\n",
      "    '''\n",
      "    #end\n",
      "    \n",
      "    def parse_config(self):\n",
      "      config = {}\n",
      "      # from file args\n",
      "      if os.path.exists('config.json'):\n",
      "          with open('config.json') as f:\n",
      "              config.update(json.load(f))\n",
      "      else:\n",
      "          # may be from command line\n",
      "          parser = argparse.ArgumentParser()\n",
      "\n",
      "          parser.add_argument('-ck', '--consumer_key', default=None, help='Your developper `Consumer Key`')\n",
      "          parser.add_argument('-cs', '--consumer_secret', default=None, help='Your developper `Consumer Secret`')\n",
      "          parser.add_argument('-at', '--access_token', default=None, help='A client `Access Token`')\n",
      "          parser.add_argument('-ats', '--access_token_secret', default=None, help='A client `Access Token Secret`')\n",
      "\n",
      "          args_ = parser.parse_args()\n",
      "          def val(key):\n",
      "            return config.get(key)\\\n",
      "                   or getattr(args_, key)\\\n",
      "                   or raw_input('Your developper `%s`: ' % key)\n",
      "          config.update({\n",
      "            'consumer_key': val('consumer_key'),\n",
      "            'consumer_secret': val('consumer_secret'),\n",
      "            'access_token': val('access_token'),\n",
      "            'access_token_secret': val('access_token_secret'),\n",
      "          })\n",
      "      # should have something now\n",
      "      return config\n",
      "\n",
      "    def oauth_req(self, url, http_method=\"GET\", post_body=None,\n",
      "                  http_headers=None):\n",
      "      config = self.parse_config()\n",
      "      consumer = oauth2.Consumer(key=config.get('consumer_key'), secret=config.get('consumer_secret'))\n",
      "      token = oauth2.Token(key=config.get('access_token'), secret=config.get('access_token_secret'))\n",
      "      request_token_url = \"http://twitter.com/oauth/request_token\"\n",
      "     \n",
      "\t\n",
      "      client = oauth2.Client(consumer, token)\n",
      "      \n",
      "      \n",
      "      resp, content = client.request(\n",
      "          url,\n",
      "          method=http_method, \n",
      "          body=post_body or '',\n",
      "          headers=http_headers,\n",
      "\t  \n",
      "      )\n",
      "      return content\n",
      "    \n",
      "    #start getTwitterData\n",
      "    def getData(self, keyword, params = {}):\n",
      "        maxTweets = 50\n",
      "        url = 'https://api.twitter.com/1.1/search/tweets.json?'    \n",
      "        data = {'q': keyword, 'lang': 'en', 'result_type': 'recent', 'count': maxTweets, 'include_entities': 0}\n",
      "\n",
      "        #Add if additional params are passed\n",
      "        if params:\n",
      "            for key, value in params.iteritems():\n",
      "                data[key] = value\n",
      "        \n",
      "        url += urllib.urlencode(data)\n",
      "        \n",
      "        response = self.oauth_req(url)\n",
      "        jsonData = json.loads(response)\n",
      "        tweets = []\n",
      "\tprint \"here\"\n",
      "        if 'errors' in jsonData:\n",
      "            print \"API Error\"\n",
      "            print jsonData['errors']\n",
      "        else:\n",
      "            for item in jsonData['statuses']:\n",
      "                tweets.append(item['text'])            \n",
      "        return tweets      \n",
      "    #end    \n",
      "\n",
      "#end class"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import regex\n",
      "import re\n",
      "import csv\n",
      "import pprint\n",
      "import nltk.classify\n",
      "#import get_twitter_data\n",
      "import json\n",
      "\n",
      "#start replaceTwoOrMore\n",
      "def replaceTwoOrMore(s):\n",
      "    #look for 2 or more repetitions of character\n",
      "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
      "    return pattern.sub(r\"\\1\\1\", s)\n",
      "#end\n",
      "\n",
      "#start process_tweet\n",
      "def processTweet(tweet):\n",
      "    # process the tweets\n",
      "    \n",
      "    #Convert to lower case\n",
      "    tweet = tweet.lower()\n",
      "    #Convert www.* or https?://* to URL\n",
      "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
      "    #Convert @username to AT_USER\n",
      "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)    \n",
      "    #Remove additional white spaces\n",
      "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
      "    #Replace #word with word\n",
      "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
      "    #trim\n",
      "    tweet = tweet.strip('\\'\"')\n",
      "    return tweet\n",
      "#end \n",
      "\n",
      "#start getStopWordList\n",
      "def getStopWordList(stopWordListFileName):\n",
      "    #read the stopwords\n",
      "    stopWords = []\n",
      "    stopWords.append('AT_USER')\n",
      "    stopWords.append('URL')\n",
      "\n",
      "    fp = open(stopWordListFileName, 'r')\n",
      "    line = fp.readline()\n",
      "    while line:\n",
      "        word = line.strip()\n",
      "        stopWords.append(word)\n",
      "        line = fp.readline()\n",
      "    fp.close()\n",
      "    return stopWords\n",
      "#end\n",
      "\n",
      "#start getfeatureVector\n",
      "def getFeatureVector(tweet, stopWords):\n",
      "    featureVector = []  \n",
      "    words = tweet.split()\n",
      "    for w in words:\n",
      "        #replace two or more with two occurrences \n",
      "        w = replaceTwoOrMore(w) \n",
      "        #strip punctuation\n",
      "        w = w.strip('\\'\"?,.')\n",
      "        #check if it consists of only words\n",
      "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*[a-zA-Z]+[a-zA-Z0-9]*$\", w)\n",
      "        #ignore if it is a stopWord\n",
      "        if(w in stopWords or val is None):\n",
      "            continue\n",
      "        else:\n",
      "            featureVector.append(w.lower())\n",
      "    return featureVector    \n",
      "#end\n",
      "\n",
      "#start extract_features\n",
      "def extract_features(tweet):\n",
      "    tweet_words = set(tweet)\n",
      "    features = {}\n",
      "    for word in featureList:\n",
      "        features['contains(%s)' % word] = (word in tweet_words)\n",
      "    return features\n",
      "#end\n",
      "\n",
      "\n",
      "#Read the tweets one by one and process it\n",
      "inpTweets = csv.reader(open('data/full_training_dataset.csv', 'rb'), delimiter=',', quotechar='|')\n",
      "stopWords = getStopWordList('data/feature_list/stopwords.txt')\n",
      "count = 0;\n",
      "featureList = []\n",
      "tweets = []\n",
      "cnt = 0;\n",
      "for row in inpTweets:\n",
      "    #left_text = row.partition(\",\")[0]\n",
      "    #print(left_text)\t\n",
      "    sentiment = row[0]\n",
      "    tweet = row[1]\n",
      "    cnt = cnt + 1\n",
      "    if cnt == 16000:\n",
      "\tbreak;\n",
      "    print(tweet)\n",
      "    processedTweet = processTweet(tweet)\n",
      "    featureVector = getFeatureVector(processedTweet, stopWords)\n",
      "    featureList.extend(featureVector)\n",
      "    tweets.append((featureVector, sentiment));\n",
      "#end loop\n",
      "\n",
      "# Remove featureList duplicates\n",
      "featureList = list(set(featureList))\n",
      "print \"done1\"\n",
      "# Generate the training set\n",
      "training_set = nltk.classify.util.apply_features(extract_features, tweets)\n",
      "print \"done2\"\n",
      "# Train the Naive Bayes classifier\n",
      "NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
      "print \"done3\"\n",
      "# Test the classifier\n",
      "for i in range(0,15):\n",
      "\tprint \"keyword\"\n",
      "\tkeyword = raw_input()\n",
      "\t#keyword = 'obama'\n",
      "\ttime = 'today'\n",
      "\ttwitterData = get_twitter_data.TwitterData()\n",
      "\ttweets = twitterData.getTwitterData(keyword, time)\n",
      "\tprint tweets\n",
      "\tprint json.dumps(tweets, indent=1)\n",
      "\n",
      "\tfor testTweet in tweets:\n",
      "\t\tprint \"this is\"\n",
      "\t\tprint testTweet\n",
      "\t\t#testTweet = \"I hate mia\"\n",
      "\t\tprocessedTestTweet = processTweet(testTweet)\n",
      "\t\tsentiment = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet, stopWords)))\n",
      "\t\tprint \"testTweet = %s, sentiment = %s\\n\" % (testTweet, sentiment)\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'data/full_training_dataset.csv'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-1ceea71d1497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;31m#Read the tweets one by one and process it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m \u001b[0minpTweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/full_training_dataset.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[0mstopWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetStopWordList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/feature_list/stopwords.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/full_training_dataset.csv'"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}